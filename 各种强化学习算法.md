各种强化学习算法





重要性采样 

Qlearning，DQN，DDPG等off policy的算法不需要importance sampling。





目前  SAC 和PPO最优.



#### 免模型学习中要学习什么

**策略优化**  和Q-Learning 是两种model-free

基于策略优化的方法举例：

- [A2C / A3C](https://arxiv.org/abs/1602.01783), 通过梯度下降直接最大化性能
- [PPO](https://arxiv.org/abs/1707.06347) , 不直接通过最大化性能更新，而是最大化 **目标估计** 函数，这个函数是目标函数 的近似估计.



**策略优化和 Q-Learning 的权衡** ：策略优化的主要优势在于这类方法是原则性的，某种意义上讲，你是直接在优化你想要的东西。与此相反，Q-learning 方法通过这种方法有很多失败的情况，所以相对来说稳定性较差.  但是，Q-learning 有效的时候能获得更好的采样效率，因为它们能够比策略优化更加有效地重用历史数据。

**策略优化和 Q-learning 的融合方法** ：意外的是，策略优化和 Q-learning 并不是不能兼容的（在某些场景下，它们两者是 [等价的](https://arxiv.org/abs/1704.06440) ），并且存在很多介于两种极端之间的算法。这个范围的算法能够很好的平衡好两者之间的优点和缺点，比如说：SAC是一种变体，它使用随机策略、熵正则化和一些其它技巧来稳定学习，同时在 benchmarks 上获得比 DDPG 更高的分



### 策略梯度 PG

连续的输出, 用一个函数来表示.



#### PG的优化方法

TRPO, ACER. 研究人员们已经找到了 [TRPO](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1502.05477)（Trust Region Policy Optimization，信任区域策略优化）和 [ACER](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.01224)（Sample Efficient Actor-Critic with Experience Replay） 这样的方法. 但是也付出了代价, ACER 比 PPO复杂得多，需要额外的代码用于策略外的纠正以及一个回放缓冲区，在 Atari 测试中的具体表现却只比 PPO 勉强好一点点；TRPO 虽然在连续控制任务中很有用，但是对策略函数和价值函数或者辅助损失之间有共享参数的算法较难兼容，比如 Atari 和其它一些视觉输入占据主要部分的任务就是这样。

### PPO

1. 将环境信息s输入到actor-new网络， 得到两个值， 一个是mu， 一个是sigma， 然后将这两个值分别当作正态分布的均值和方差构建正态分布(意义是表示action的分布)，然后通过这个正态分布sample出来一个action， 再输入到环境中得到奖励r和下一步的状态s_，然后存储[(s,a,r),…], 再将s_输入到actor-new网络，循环步骤1， 直到存储了一定量的[(s, a, r), …]， 注意这个过程中actor-new网络没有更新。这应该是一个MDP 马尔可夫决策过程.

2. 将1循环完最后一步得到的s输入到critic-NN网络中， 得到状态的v_值， 然后计算折扣奖励

3. 将存储的所有s组合输入到critic-NN网络中， 得到所有状态的V_值， 计算At = R – V 

4. 求c_loss = mean(square(At )), 然后反向传播更新critic-NN网络。

5. 将存储的所有s组合输入actor-old和actor-new网络(网络结构一样)， 分别得到正态分布Normal1和Normal2， 将存储的所有action组合为actions输入到正态分布Normal1和Normal2， 得到每个actions对应的prob1和prob2， 然后用prob2除以prob1得到important weight, 也就是ratio。

6. 根据论文公式7计算a_loss = mean(min((ration* At, clip(ratio, 1-ξ, 1+ξ)* At))), 然后反向传播， 更新actor-new网络。

7. 循环5-6步骤， 一定步后， 循环结束， 用actor-new网络权重来更新actor-old网络(莫凡代码是在循环开始前更新的， 效果是一样的)。

8. 循环1-7步骤。


曾yiyan 的代码, 大规模DRL时 需要 actor loss 更新actor网络,  critic loss 更新 critic 网络.  节省显存. 
