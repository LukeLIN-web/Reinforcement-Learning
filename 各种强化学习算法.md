各种强化学习算法





重要性采样 

Qlearning，DQN，DDPG等off policy的算法不需要importance sampling。





目前  SAC 和PPO最优.



#### 免模型学习中要学习什么

**策略优化**  和Q-Learning 是两种model-free

基于策略优化的方法举例：

- [A2C / A3C](https://arxiv.org/abs/1602.01783), 通过梯度下降直接最大化性能
- [PPO](https://arxiv.org/abs/1707.06347) , 不直接通过最大化性能更新，而是最大化 **目标估计** 函数，这个函数是目标函数 的近似估计.



**策略优化和 Q-Learning 的权衡** ：策略优化的主要优势在于这类方法是原则性的，某种意义上讲，你是直接在优化你想要的东西。与此相反，Q-learning 方法通过这种方法有很多失败的情况，所以相对来说稳定性较差.  但是，Q-learning 有效的时候能获得更好的采样效率，因为它们能够比策略优化更加有效地重用历史数据。

**策略优化和 Q-learning 的融合方法** ：意外的是，策略优化和 Q-learning 并不是不能兼容的（在某些场景下，它们两者是 [等价的](https://arxiv.org/abs/1704.06440) ），并且存在很多介于两种极端之间的算法。这个范围的算法能够很好的平衡好两者之间的优点和缺点，比如说：SAC是一种变体，它使用随机策略、熵正则化和一些其它技巧来稳定学习，同时在 benchmarks 上获得比 DDPG 更高的分

### 

连续的输出, 用一个函数来表示.

### 策略梯度 PG

[第四章 策略梯度 (datawhalechina.github.io)](https://datawhalechina.github.io/easy-rl/#/chapter4/chapter4)

首先, 最重要的是 (st, at) 这个tuple, 也就是 t时刻的state 和action

我们不知道 下一个时刻的state,  我们的模型是 在state t下, 概率选择各个action.

我们需要修改theta参数,  让累计reward 的期望越大越好,

怎么最大化期望奖励呢？我们用的是 `梯度上升(gradient ascent)`，因为要让它越大越好，所以是梯度上升。梯度上升在更新参数的时候要加。要进行梯度上升，我们先要计算期望的奖励(expected reward)的梯度。

这里略去一堆数学推导, 也就是期望和trajectory 有关. 

实际上这个期望值没有办法算，所以你是用采样的方式来采样一大堆的trajectory。你采样 N 笔 trajectory ， 然后你去计算每一笔的这些值，然后把它全部加起来，就可以得到梯度。你就可以去更新参数，你就可以去更新你的 agent.

你用梯度上升来更新你的参数，你原来有一个参数 θ ，把你的 θ 加上你的梯度这一项，那当然前面要有个学习率，学习率也是要调整的，你可用 Adam、RMSProp 等方法对其进行调整。

要套上面这个公式， 首先你要先收集一大堆的 s 跟 a 的对(pair)，你还要知道这些 s 跟 a 在跟环境互动的时候，你会得到多少的奖励。 这些资料怎么收集呢？

你要拿你的 agent，它的参数是 θ，去跟环境做互动， 也就是拿你已经训练好的 agent 先去跟环境玩一下，先去跟那个游戏互动一下， 互动完以后，你就会得到一大堆游戏的纪录

你就可以把采样到的东西代到这个梯度的式子里面，把梯度算出来。每一个 s 跟 a 的pair拿进来，算一下它的对数概率(log probability)。你计算一下在某一个状态采取某一个动作的对数概率，然后对它取梯度，然后这个梯度前面会乘一个权重，权重就是这场游戏的奖励。 有了这些以后，你就会去更新你的模型。

更新完你的模型以后。你要重新去收集数据，再更新模型。注意，一般 `policy gradient(PG) `采样的数据就只会用一次。你把这些数据采样起来，然后拿去更新参数，这些数据就丢掉了。接着再重新采样数据，才能够去更新参数，等一下我们会解决这个问题。(好像也没说, 我觉得应该是  经验重放replay)

#### PG的优化方法

1. **第一个 tip 是  baseline。** 如果给定状态 s 采取动作 a 会给你整场游戏正的奖励，就要增加它的概率。如果状态 s 执行动作 a，整场游戏得到负的奖励，就要减少这一项的概率。但在很多游戏里面，奖励总是正的，就是说最低都是 0。比如说打乒乓球游戏， 你的分数就是介于 0 到 21 分之间，所以 R 总是正的。假设你直接套用这个式子， 在训练的时候告诉模型说，不管是什么动作你都应该要把它的概率提升。 在理想上，这么做并不一定会有问题。因为虽然说 R 总是正的，但它正的量总是有大有小，你在玩乒乓球那个游戏里面，得到的奖励总是正的，但它是介于 0~21分之间，有时候你采取某些动作可能是得到 0 分，采取某些动作可能是得到 20 分。 a 不一定是一个不好的动作， 它只是没被采样到。但只是因为它没被采样到， 它的概率就会下降，这个显然是有问题的，要怎么解决这个问题呢？你会希望你的奖励不要总是正的。  所以要减去 baseline.把 R−b 这一项合起来，我们统称为` 优势函数(advantage function)`， 用 `A` 来代表优势函数。优势函数取决于 s 和 a，我们就是要计算的是在某一个状态 s 采取某一个动作 a 的时候，优势函数有多大。 critic 就是估计advantage的.

2. 第二个是  动作权重 .我们希望可以给每一个不同的动作前面都乘上不同的权重。每一个动作的不同权重， 它反映了每一个动作到底是好还是不好。在采样的次数不够多的情况下，你要给每一个状态跟动作对合理的分数，你要让大家知道它合理的贡献。怎么给它一个合理的贡献呢？

   一个做法是计算这个对的奖励的时候，不把整场游戏得到的奖励全部加起来，**只计算从这一个动作执行以后所得到的奖励**。因为这场游戏在执行这个动作之前发生的事情是跟执行这个动作是没有关系的， 所以在执行这个动作之前得到多少奖励都不能算是这个动作的功劳。

   

 ![](https://datawhalechina.github.io/easy-rl/chapter4/img/4.21.png)

我们介绍下策略梯度最简单的也是最经典的一个算法 `REINFORCE`。REINFORCE 用的是回合更新的方式。它在代码上的处理上是先拿到每个步骤的奖励，然后计算每个步骤的未来总收益 G_t 是多少，然后拿每个 G_t 代入公式，去优化每一个动作的输出。所以编写代码时会有这样一个函数，输入每个步骤拿到的奖励，把这些奖励转成每一个步骤的未来总收益Gt。 这个就是对动作的评价.

实际的动作编码为  one-hot 向量(类似label), 神经网络输出预测动作的概率（类似于output）.实际的动作 对比label, 因为不是完全正确的动作, 所以loss还需要乘上一个对动作的评价Gt.

因为我们会拿到整个回合的所有的轨迹，所以我们可以对这一条整条轨迹里面的每个动作都去计算一个 loss。把所有的 loss 加起来之后，我们再扔给 adam 的优化器去自动更新参数就好了。

![img](https://datawhalechina.github.io/easy-rl/chapter4/img/4.28.png)

上图是 REINFORCE 的流程图。首先我们需要一个 policy model 来输出动作概率，输出动作概率后，我们 `sample()` 函数去得到一个具体的动作，然后跟环境交互过后，我们可以得到一整个回合的数据。拿到回合数据之后，我再去执行一下 `learn()` 函数，在 `learn()` 函数里面，我就可以拿这些数据去构造损失函数，扔给这个优化器去优化，去更新我的 policy model。



- 对于梯度策略的两种方法，蒙特卡洛（MC）强化学习和时序差分（TD）强化学习两个方法有什么联系和区别？

  答：

  1. **两者的更新频率不同**，蒙特卡洛强化学习方法是**每一个episode更新一次**，即需要经历完整的状态序列后再更新（比如我们的贪吃蛇游戏，贪吃蛇“死了”游戏结束后再更新），而对于时序差分强化学习方法是**每一个step就更新一次** ，（比如我们的贪吃蛇游戏，贪吃蛇每移动一次（或几次）就进行更新）。相对来说，时序差分强化学习方法比蒙特卡洛强化学习方法更新的频率更快。
  2. 时序差分强化学习能够在知道一个小step后就进行学习，相比于蒙特卡洛强化学习，其更加**快速、灵活**。
  3. 具体举例来说：假如我们要优化开车去公司的通勤时间。对于此问题，每一次通勤，我们将会到达不同的路口。对于时序差分（TD）强化学习，其会对于每一个经过的路口都会计算时间，例如在路口 A 就开始更新预计到达路口 B、路口 C \cdots \cdots⋯⋯, 以及到达公司的时间；而对于蒙特卡洛（MC）强化学习，其不会每经过一个路口就更新时间，而是到达最终的目的地后，再修改每一个路口和公司对应的时间。

TRPO, ACER. 研究人员们已经找到了 [TRPO](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1502.05477)（Trust Region Policy Optimization，信任区域策略优化）和 [ACER](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.01224)（Sample Efficient Actor-Critic with Experience Replay） 这样的方法. 但是也付出了代价, ACER 比 PPO复杂得多，需要额外的代码用于策略外的纠正以及一个回放缓冲区，在 Atari 测试中的具体表现却只比 PPO 勉强好一点点；TRPO 虽然在连续控制任务中很有用，但是对策略函数和价值函数或者辅助损失之间有共享参数的算法较难兼容，比如 Atari 和其它一些视觉输入占据主要部分的任务就是这样。

### PPO

1. 将环境信息s输入到actor-new网络， 得到两个值， 一个是mu， 一个是sigma， 然后将这两个值分别当作正态分布的均值和方差构建正态分布(意义是表示action的分布)，然后通过这个正态分布sample出来一个action， 再输入到环境中得到奖励r和下一步的状态s_，然后存储[(s,a,r),…], 再将s_输入到actor-new网络，循环步骤1， 直到存储了一定量的[(s, a, r), …]， 注意这个过程中actor-new网络没有更新。这应该是一个MDP 马尔可夫决策过程.

2. 将1循环完最后一步得到的s输入到critic-NN网络中， 得到状态的v_值， 然后计算折扣奖励

3. 将存储的所有s组合输入到critic-NN网络中， 得到所有状态的V_值， 计算At = R – V 

4. 求c_loss = mean(square(At )), 然后反向传播更新critic-NN网络。

5. 将存储的所有s组合输入actor-old和actor-new网络(网络结构一样)， 分别得到正态分布Normal1和Normal2， 将存储的所有action组合为actions输入到正态分布Normal1和Normal2， 得到每个actions对应的prob1和prob2， 然后用prob2除以prob1得到important weight, 也就是ratio。

6. 根据论文公式7计算a_loss = mean(min((ration* At, clip(ratio, 1-ξ, 1+ξ)* At))), 然后反向传播， 更新actor-new网络。

7. 循环5-6步骤， 一定步后， 循环结束， 用actor-new网络权重来更新actor-old网络(莫凡代码是在循环开始前更新的， 效果是一样的)。

8. 循环1-7步骤。


曾yiyan 的代码, 大规模DRL时 需要 actor loss 更新actor网络,  critic loss 更新 critic 网络.  节省显存. 
